#!/usr/bin/env python3
"""
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼
Easy Scraperã¨é€£æºã—ã¦æŠ•ç¨¿ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–ã—ã€è‡ªå‹•çš„ã«é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‚’å­¦ç¿’
"""

import os
import time
import json
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import schedule
import threading
import numpy as np

class RealtimeEngagementTracker:
    def __init__(self):
        self.db_path = "threads_auto_post.db"
        self.spreadsheet_id = "1jdGRxpyM4n2Tri41AK7jn-tM6GbRZc6MYIStsco7gNs"
        self.threads_username = "seisato0829"
        self.check_interval = 30  # 30åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
        self.engagement_history = []
        
    def setup_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸè¨­å®š"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS engagement_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_url TEXT UNIQUE,
            content TEXT,
            impressions INTEGER DEFAULT 0,
            likes INTEGER DEFAULT 0,
            comments INTEGER DEFAULT 0,
            reposts INTEGER DEFAULT 0,
            saves INTEGER DEFAULT 0,
            engagement_rate REAL,
            checked_at TIMESTAMP,
            posted_at TIMESTAMP,
            is_high_performer BOOLEAN DEFAULT 0
        )
        """)
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS learning_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            pattern_type TEXT,
            pattern_value TEXT,
            success_rate REAL,
            usage_count INTEGER DEFAULT 1,
            last_updated TIMESTAMP
        )
        """)
        
        conn.commit()
        conn.close()
    
    def scrape_threads_data(self) -> List[Dict]:
        """Seleniumã‚’ä½¿ã£ã¦Threadsãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        driver = webdriver.Chrome(options=chrome_options)
        posts_data = []
        
        try:
            # Threadsãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            url = f"https://www.threads.net/@{self.threads_username}"
            driver.get(url)
            
            # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "article"))
            )
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æŠ•ç¨¿ã‚’èª­ã¿è¾¼ã‚€
            last_height = driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            
            while scroll_attempts < 5:  # æœ€å¤§5å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                    
                last_height = new_height
                scroll_attempts += 1
            
            # æŠ•ç¨¿ã‚’å–å¾—
            posts = driver.find_elements(By.TAG_NAME, "article")
            
            for post in posts[:20]:  # æœ€æ–°20æŠ•ç¨¿ã‚’å–å¾—
                try:
                    post_data = self._extract_post_data(post, driver)
                    if post_data:
                        posts_data.append(post_data)
                except Exception as e:
                    print(f"æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                    
        finally:
            driver.quit()
            
        return posts_data
    
    def _extract_post_data(self, post_element, driver) -> Dict:
        """æŠ•ç¨¿è¦ç´ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        data = {}
        
        try:
            # æŠ•ç¨¿URL
            link_element = post_element.find_element(By.CSS_SELECTOR, "a[href*='/post/']")
            data['url'] = link_element.get_attribute('href')
            
            # æŠ•ç¨¿å†…å®¹
            content_element = post_element.find_element(By.CSS_SELECTOR, "[data-testid='post-content']")
            data['content'] = content_element.text
            
            # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°å€¤ã‚’å–å¾—
            # ã„ã„ã­æ•°
            try:
                likes_element = post_element.find_element(By.CSS_SELECTOR, "[aria-label*='like']")
                data['likes'] = self._extract_number(likes_element.text)
            except:
                data['likes'] = 0
                
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
            try:
                comments_element = post_element.find_element(By.CSS_SELECTOR, "[aria-label*='comment']")
                data['comments'] = self._extract_number(comments_element.text)
            except:
                data['comments'] = 0
                
            # ãƒªãƒã‚¹ãƒˆæ•°
            try:
                reposts_element = post_element.find_element(By.CSS_SELECTOR, "[aria-label*='repost']")
                data['reposts'] = self._extract_number(reposts_element.text)
            except:
                data['reposts'] = 0
            
            # æŠ•ç¨¿æ™‚é–“
            time_element = post_element.find_element(By.TAG_NAME, "time")
            data['posted_at'] = time_element.get_attribute('datetime')
            
            # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã¯æ¨å®šå€¤ã‚’ä½¿ç”¨
            data['impressions'] = self._estimate_impressions(data)
            
            # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’è¨ˆç®—
            total_engagement = data['likes'] + data['comments'] + data['reposts']
            data['engagement_rate'] = total_engagement / max(data['impressions'], 1)
            
            data['checked_at'] = datetime.now().isoformat()
            
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
        return data
    
    def _extract_number(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
        import re
        
        if not text:
            return 0
            
        # K, Mè¡¨è¨˜ã‚’å‡¦ç†
        text = text.upper()
        if 'K' in text:
            num = float(re.findall(r'[\d.]+', text)[0])
            return int(num * 1000)
        elif 'M' in text:
            num = float(re.findall(r'[\d.]+', text)[0])
            return int(num * 1000000)
        else:
            nums = re.findall(r'\d+', text)
            return int(nums[0]) if nums else 0
    
    def _estimate_impressions(self, post_data: Dict) -> int:
        """ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’æ¨å®š"""
        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•° Ã— æŠ•ç¨¿çµŒéæ™‚é–“ã«ã‚ˆã‚‹æ¸›è¡°ç‡
        base_followers = 1000  # åŸºæœ¬ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ï¼ˆå®Ÿéš›ã®å€¤ã«ç½®ãæ›ãˆï¼‰
        
        # æŠ•ç¨¿ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
        if post_data.get('posted_at'):
            posted_time = datetime.fromisoformat(post_data['posted_at'].replace('Z', '+00:00'))
            hours_passed = (datetime.now() - posted_time).total_seconds() / 3600
            
            # æ™‚é–“ã«ã‚ˆã‚‹æ¸›è¡°ï¼ˆæœ€åˆã®24æ™‚é–“ã§90%ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼‰
            decay_rate = max(0.1, 1 - (hours_passed / 24) * 0.9)
        else:
            decay_rate = 0.5
            
        estimated = int(base_followers * decay_rate * random.uniform(0.8, 1.2))
        return max(estimated, 100)  # æœ€ä½100ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
    
    def update_engagement_data(self, posts_data: List[Dict]):
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for post in posts_data:
            # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¤å®šï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡5%ä»¥ä¸Šï¼‰
            is_high_performer = post['engagement_rate'] >= 0.05
            
            cursor.execute("""
            INSERT OR REPLACE INTO engagement_history 
            (post_url, content, impressions, likes, comments, reposts, 
             engagement_rate, checked_at, posted_at, is_high_performer)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                post['url'], post['content'], post['impressions'],
                post['likes'], post['comments'], post['reposts'],
                post['engagement_rate'], post['checked_at'],
                post.get('posted_at'), is_high_performer
            ))
            
            # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‹ã‚‰å­¦ç¿’
            if is_high_performer:
                self._learn_from_post(post, cursor)
        
        conn.commit()
        conn.close()
    
    def _learn_from_post(self, post: Dict, cursor):
        """é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‹ã‚‰å­¦ç¿’"""
        content = post['content']
        
        # çµµæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        import re
        emojis = re.findall(r'[^\u0000-\u007F\u0080-\u00FF\u2000-\u206F]+', content)
        for emoji in emojis:
            self._update_learning_data(cursor, 'emoji', emoji, post['engagement_rate'])
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å­¦ç¿’
        keywords = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,}', content)
        for keyword in keywords[:10]:  # ä¸Šä½10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            self._update_learning_data(cursor, 'keyword', keyword, post['engagement_rate'])
        
        # æ–‡æ§‹é€ ã‚’å­¦ç¿’
        if 'ï¼Ÿ' in content or '?' in content:
            self._update_learning_data(cursor, 'structure', 'question', post['engagement_rate'])
        if re.search(r'\d+\.', content):
            self._update_learning_data(cursor, 'structure', 'numbered_list', post['engagement_rate'])
        if 'âœ…' in content or 'ãƒ»' in content:
            self._update_learning_data(cursor, 'structure', 'bullet_points', post['engagement_rate'])
    
    def _update_learning_data(self, cursor, pattern_type: str, pattern_value: str, success_rate: float):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        cursor.execute("""
        INSERT INTO learning_data (pattern_type, pattern_value, success_rate, last_updated)
        VALUES (?, ?, ?, ?)
        ON CONFLICT(pattern_type, pattern_value) DO UPDATE SET
        success_rate = (success_rate * usage_count + ?) / (usage_count + 1),
        usage_count = usage_count + 1,
        last_updated = ?
        """, (pattern_type, pattern_value, success_rate, datetime.now().isoformat(),
              success_rate, datetime.now().isoformat()))
    
    def get_learning_insights(self) -> Dict:
        """å­¦ç¿’ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ´å¯Ÿã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        
        insights = {
            'top_emojis': pd.read_sql_query("""
                SELECT pattern_value, success_rate, usage_count
                FROM learning_data
                WHERE pattern_type = 'emoji'
                ORDER BY success_rate DESC
                LIMIT 10
            """, conn).to_dict('records'),
            
            'top_keywords': pd.read_sql_query("""
                SELECT pattern_value, success_rate, usage_count
                FROM learning_data
                WHERE pattern_type = 'keyword'
                ORDER BY success_rate DESC
                LIMIT 20
            """, conn).to_dict('records'),
            
            'effective_structures': pd.read_sql_query("""
                SELECT pattern_value, success_rate, usage_count
                FROM learning_data
                WHERE pattern_type = 'structure'
                ORDER BY success_rate DESC
            """, conn).to_dict('records')
        }
        
        conn.close()
        return insights
    
    def export_to_spreadsheet(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’Google Spreadsheetã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        conn = sqlite3.connect(self.db_path)
        
        # æœ€æ–°ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = pd.read_sql_query("""
            SELECT * FROM engagement_history
            WHERE checked_at >= datetime('now', '-7 days')
            ORDER BY engagement_rate DESC
        """, conn)
        
        # CSVã¨ã—ã¦ä¿å­˜ï¼ˆGoogle Sheetsã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ï¼‰
        export_path = f"engagement_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(export_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {export_path}")
        
        conn.close()
        return export_path
    
    def run_monitoring(self):
        """ç›£è¦–ã‚’å®Ÿè¡Œ"""
        print("ğŸ” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™...")
        
        def job():
            print(f"\nğŸ“Š {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ãƒã‚§ãƒƒã‚¯é–‹å§‹")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            posts_data = self.scrape_threads_data()
            print(f"âœ… {len(posts_data)}ä»¶ã®æŠ•ç¨¿ã‚’å–å¾—")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
            self.update_engagement_data(posts_data)
            
            # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‚’è¡¨ç¤º
            high_performers = [p for p in posts_data if p['engagement_rate'] >= 0.05]
            if high_performers:
                print(f"ğŸŒŸ é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿: {len(high_performers)}ä»¶")
                for post in high_performers[:3]:
                    print(f"   - ER: {post['engagement_rate']*100:.1f}% | {post['content'][:30]}...")
            
            # å­¦ç¿’æ´å¯Ÿã‚’è¡¨ç¤º
            insights = self.get_learning_insights()
            if insights['top_keywords']:
                print(f"ğŸ”¥ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join([k['pattern_value'] for k in insights['top_keywords'][:5]])}")
        
        # åˆå›å®Ÿè¡Œ
        self.setup_database()
        job()
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
        schedule.every(self.check_interval).minutes.do(job)
        
        # æ¯æ—¥ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        schedule.every().day.at("09:00").do(self.export_to_spreadsheet)
        
        # å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
        while True:
            schedule.run_pending()
            time.sleep(60)

if __name__ == "__main__":
    tracker = RealtimeEngagementTracker()
    tracker.run_monitoring()