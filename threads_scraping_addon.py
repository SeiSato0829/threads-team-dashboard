#!/usr/bin/env python3
"""
Threadsè‡ªå‹•ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¢ãƒ‰ã‚ªãƒ³
æ—¢å­˜ã®åŽç›ŠåŒ–ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import requests
import csv
import json
from datetime import datetime
import time
import os

class ThreadsScrapingAddon:
    def __init__(self):
        self.access_token = ""  # Threads Graph API ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.user_id = ""       # ã‚ãªãŸã®Threadsãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        self.csv_file = "money_optimization_sheets/01_åŽç›Šãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°.tsv"
        
    def setup_api_credentials(self):
        """APIèªè¨¼æƒ…å ±ã®è¨­å®š"""
        print("ðŸ”‘ Threads APIè¨­å®š")
        print("=" * 50)
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
        self.access_token = os.getenv('THREADS_ACCESS_TOKEN')
        self.user_id = os.getenv('THREADS_USER_ID')
        
        if not self.access_token:
            print("âš ï¸ THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ä»¥ä¸‹ã®æ‰‹é †ã§APIã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¦ãã ã•ã„ï¼š")
            print()
            print("1. Meta for Developers (developers.facebook.com) ã«ã‚¢ã‚¯ã‚»ã‚¹")
            print("2. ã‚¢ãƒ—ãƒªã‚’ä½œæˆ")
            print("3. Threads Basic Display API ã‚’æœ‰åŠ¹åŒ–")
            print("4. ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ")
            print()
            
            # æ‰‹å‹•å…¥åŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            self.access_token = input("ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ï¼ˆç©ºç™½ã®å ´åˆã¯æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼‰: ").strip()
            
        if not self.user_id:
            self.user_id = input("Threadsãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å…¥åŠ›: ").strip()
            
        return bool(self.access_token and self.user_id)
    
    def get_threads_data(self, post_url=None):
        """ThreadsæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        
        if not self.access_token:
            print("âŒ APIè¨­å®šãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
            return self._manual_data_entry()
            
        try:
            # Threads Graph APIå‘¼ã³å‡ºã—
            url = f"https://graph.threads.net/{self.user_id}/threads"
            params = {
                'fields': 'id,media_type,text,timestamp,like_count,reply_count,repost_count,views',
                'access_token': self.access_token,
                'limit': 10
            }
            
            response = requests.get(url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                return self._process_api_data(data)
            else:
                print(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return self._manual_data_entry()
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return self._manual_data_entry()
    
    def _process_api_data(self, api_data):
        """APIãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        posts = []
        
        for post in api_data.get('data', []):
            processed_post = {
                'id': post.get('id'),
                'content': post.get('text', '')[:50] + '...',
                'timestamp': post.get('timestamp'),
                'likes': post.get('like_count', 0),
                'comments': post.get('reply_count', 0),
                'reposts': post.get('repost_count', 0),
                'views': post.get('views', 0)
            }
            posts.append(processed_post)
            
        return posts
    
    def _manual_data_entry(self):
        """æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰"""
        print("ðŸ“± æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰")
        print("æŠ•ç¨¿ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        
        post_data = {
            'content': input("æŠ•ç¨¿å†…å®¹ï¼ˆä¸€éƒ¨ï¼‰: "),
            'likes': int(input("ã„ã„ã­æ•°: ") or 0),
            'comments': int(input("ã‚³ãƒ¡ãƒ³ãƒˆæ•°: ") or 0),
            'reposts': int(input("ãƒªãƒã‚¹ãƒˆæ•°: ") or 0),
            'profile_clicks': int(input("ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¯ãƒªãƒƒã‚¯æ•°: ") or 0),
            'new_followers': int(input("æ–°è¦ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: ") or 0),
            'link_clicks': int(input("ãƒªãƒ³ã‚¯ã‚¯ãƒªãƒƒã‚¯æ•°: ") or 0),
            'revenue': float(input("å£²ä¸Šè²¢çŒ®é¡ï¼ˆå††ï¼‰: ") or 0)
        }
        
        return [post_data]
    
    def update_spreadsheet_data(self, posts_data):
        """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        
        if not os.path.exists(self.csv_file):
            print("âŒ åŽç›Šãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
            
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with open(self.csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f, delimiter='\t')
                rows = list(reader)
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            for post in posts_data:
                new_row = [
                    datetime.now().strftime('%Y/%m/%d %H:%M'),  # æŠ•ç¨¿æ—¥æ™‚
                    datetime.now().strftime('%a'),              # æ›œæ—¥
                    'ç”»åƒ',                                      # æŠ•ç¨¿ã‚¿ã‚¤ãƒ—
                    'education',                                # ã‚«ãƒ†ã‚´ãƒª
                    post.get('content', ''),                    # æŠ•ç¨¿å†…å®¹
                    '',                                         # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
                    'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯',                        # CTA
                    post.get('likes', 0),                       # ã„ã„ã­æ•°
                    post.get('comments', 0),                    # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
                    post.get('reposts', 0),                     # ã‚·ã‚§ã‚¢æ•°
                    0,                                          # DMæ•°
                    '',                                         # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡
                    post.get('profile_clicks', 0),              # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¯ãƒªãƒƒã‚¯
                    post.get('new_followers', 0),               # æ–°è¦ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼
                    post.get('link_clicks', 0),                 # ãƒªãƒ³ã‚¯ã‚¯ãƒªãƒƒã‚¯
                    post.get('revenue', 0),                     # å£²ä¸Šè²¢çŒ®é¡
                    '',                                         # ROI
                    '',                                         # CPF
                    ''                                          # ãƒ¡ãƒ¢
                ]
                rows.append(new_row)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
            with open(self.csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f, delimiter='\t')
                writer.writerows(rows)
                
            print("âœ… ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†ï¼")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_automation(self):
        """è‡ªå‹•åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨­å®š"""
        print("âš™ï¸ è‡ªå‹•åŒ–è¨­å®š")
        print("=" * 30)
        
        schedule_code = '''
# Windows ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç”¨ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«
# threads_auto_update.bat ã¨ã—ã¦ä¿å­˜

@echo off
cd C:\\Users\\music-020\\threads-auto-post
python threads_scraping_addon.py --auto
pause
'''
        
        batch_file = "threads_auto_update.bat"
        with open(batch_file, 'w', encoding='utf-8') as f:
            f.write(schedule_code)
            
        print(f"âœ… {batch_file} ã‚’ä½œæˆã—ã¾ã—ãŸ")
        print()
        print("è‡ªå‹•å®Ÿè¡Œè¨­å®š:")
        print("1. Windowsã‚­ãƒ¼ + R")
        print("2. 'taskschd.msc' ã¨å…¥åŠ›ã—ã¦Enter")
        print("3. 'åŸºæœ¬ã‚¿ã‚¹ã‚¯ã®ä½œæˆ'ã‚’ã‚¯ãƒªãƒƒã‚¯")
        print(f"4. ä½œæˆã—ãŸ {batch_file} ã‚’æŒ‡å®š")
        print("5. æ¯Žæ—¥ã®å®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®š")

def main():
    scraper = ThreadsScrapingAddon()
    
    print("ðŸš€ Threadsè‡ªå‹•ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # APIè¨­å®š
    if scraper.setup_api_credentials():
        print("âœ… APIè¨­å®šå®Œäº†")
    else:
        print("âš ï¸ æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    print("\nðŸ“Š æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    posts_data = scraper.get_threads_data()
    
    if posts_data:
        print(f"âœ… {len(posts_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°
        if scraper.update_spreadsheet_data(posts_data):
            print("âœ… åŽç›Šãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        
        # è‡ªå‹•åŒ–è¨­å®š
        setup_auto = input("\nè‡ªå‹•åŒ–ã‚’è¨­å®šã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower() == 'y'
        if setup_auto:
            scraper.setup_automation()
    
    print("\nðŸŽ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ç¢ºèª")
    print("2. ROIåˆ†æžã‚’å®Ÿè¡Œ")
    print("3. é«˜åŽç›Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--auto':
        # è‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        scraper = ThreadsScrapingAddon()
        if scraper.setup_api_credentials():
            posts_data = scraper.get_threads_data()
            if posts_data:
                scraper.update_spreadsheet_data(posts_data)
    else:
        # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
        main()